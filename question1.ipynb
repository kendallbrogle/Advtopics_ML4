{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMAdi9qgC-B9"
   },
   "source": [
    "To copy this template: File -> Save a Copy in Drive\n",
    "\n",
    "***DISCLAIMER**: In case of any discrepancy in the assignment instruction, please refer to the `PDF` document.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcDhlfqyBd6m"
   },
   "source": [
    "# Problem 1 - Convolutional Neural Networks Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8zfG5XN4tRo"
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-id00ye6CNLB"
   },
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EfcEgq1RSQv"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "Layer | Size(output) | Strides | Padding | Weight | Biases | Parameters\n",
    ":-----:|:----------:|:---:|:---------:|:-----:|:---:|:---:\n",
    "Input | 224x224x3 | - | - | - | - | 0\n",
    "Conv-1 | 55x55x96 | 4 | 0 | 11\\*11\\*3\\*96 = 34,848 | 96 | 34,848 + 96 = 34,944 \n",
    "MaxPool-1 | 27x27x96 | 2 | 0 | 0 | 0 | 0\n",
    "Conv-2 | 27x27x256 | 1 | 2 | 5\\*5\\*48\\*256 = 307,200 | 256 | 307,200 + 256 = 307,456 \n",
    "MaxPool-2 | 13X13X256 | 2 | 0 | 0 | 0 | 0\n",
    "Conv-3 | 13x13x384| 1 | 1 | 3\\*3\\*256\\*384 = 884,736 | 384 | 884,736 + 384 = 885,120\n",
    "Conv-4 | 13x13x384 | 1 | 1 | 3\\*3\\*192\\*384 = 663,552 | 384 | 663,552 + 384 = 663,936\n",
    "Conv-5 | 13x13x256 | 1 | 1 | 3\\*3\\*192\\*256 = 442,368 | 256 | 442,368 + 256 = 442,624\n",
    "MaxPool-3 | 6x6x256 | 2 | 0 | 0 | 0 | 0\n",
    "FC-1 | 4096x1 | - | - | 6\\*6\\*256\\*4096 = 37,748,736 | 4096  | 37,748,736 + 4,096 = 37,752,832 \n",
    "FC-2 | 4096x1 | - | - | 1\\*4096\\*1\\*4096 = 16,777,216 | 4096  |  16,777,216 + 4,096 = 16,781,312\n",
    "FC-3 | 1000x1 | - | - | 1\\*4096\\*1\\*1000 = 4,096,000 | 1000 | 4,096,000 + 1,000 = 4,097,000\n",
    "Output | 1000x1 | - | - | 0 | 0 | 0\n",
    "**Total** | | | | | | **60,965,224**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1D_yfneCWqL"
   },
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Layer | Number of Activations (Momery) | Parameters (Compute)\n",
    ":-----:|:----------:|:---:\n",
    "Input | 224\\*224\\*3 = 150.5K | 0\n",
    "CONV3-64 | 224\\*224\\*64 = 3.2M | (3\\*3\\*3)\\*64 = 1,728\n",
    "CONV3-64 | 224\\*224\\*64 = 3.2M | (3\\*3\\*64)\\*64 = 36,864\n",
    "POOL2 |  112\\*112\\*64 = 802.8K | 0\n",
    "CONV3-128 | 112\\*112\\*128 = 1.6M | (3\\*3\\*64)\\*128 = 73,728\n",
    "CONV3-128 | 112\\*112\\*128 = 1.6M | (3\\*3\\*128)\\*128 = 147,456\n",
    "POOL2 |  56\\*56\\*128 = 401K | 0\n",
    "CONV3-256 | 56\\*56\\*256 = 802.8K | (3\\*3\\*128)\\*256 = 294,912\n",
    "CONV3-256 | 56\\*56\\*256 = 802.8K | (3\\*3\\*256)\\*256 = 589,824\n",
    "CONV3-256 | 56\\*56\\*256 = 802.8K | (3\\*3\\*256)\\*256 = 589,824\n",
    "CONV3-256 | 56\\*56\\*256 = 802.8K | (3\\*3\\*256)\\*256 = 589,824\n",
    "POOL2 | 28\\*28\\*256= 200.7K | 0\n",
    "CONV3-512 | 28\\*28\\*512 = 401K | (3\\*3\\*256)\\*512 = 1,179,648\n",
    "CONV3-512 | 28\\*28\\*512 = 401K | (3\\*3\\*512)\\*512 = 2,359,296\n",
    "CONV3-512 | 28\\*28\\*512 = 401K | (3\\*3\\*512)\\*512 = 2,359,296\n",
    "CONV3-512 | 28\\*28\\*512 = 401K | (3\\*3\\*512)\\*512 = 2,359,296\n",
    "POOL2 | 14\\*14\\*512= 100.4K | 0\n",
    "CONV3-512 | 14\\*14\\*512= 100.4K | (3\\*3\\*512)\\*512 = 2,359,296\n",
    "CONV3-512 | 14\\*14\\*512= 100.4K | (3\\*3\\*512)\\*512 = 2,359,296\n",
    "CONV3-512 | 14\\*14\\*512= 100.4K | (3\\*3\\*512)\\*512 = 2,359,296\n",
    "CONV3-512 | 14\\*14\\*512= 100.4K | (3\\*3\\*512)\\*512 = 2,359,296\n",
    "POOL2 | 7\\*7\\*512= 25,088 | 0\n",
    "FC | 4096 | 25,088*4096 = 102,760,448\n",
    "FC | 4096 | 4096*4096 = 16,777,216\n",
    "FC | 1000 | 4096*1000 = 4,096,000\n",
    "**Total** | **16,508,888 activations** | **143,632,544 parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYpokEaLYBwY"
   },
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "Show that a stack of N convolution layers each of filter size F × F has the same\n",
    "receptive field as one convolution layer with filter of size (NF − N + 1) × (NF − N + 1). Use this to\n",
    "calculate the receptive field of 3 filters of size 5x5: \n",
    "\n",
    "The first convolution layer receptive field RF1 = F\n",
    "\n",
    "The second convolution layer is affected by filter size and groth from RF1. Therefore RF2 = RF1 + (F-1) = F + F -1 = 2F - 1 The F-1 term accounts for the overlap when the filter is applied.\n",
    "\n",
    "The third conv layer, RF3 = RF2 + (F-1) = 2F-1 + F -1 = 3F - 2\n",
    "This creates a pattern where for every layer N the RFN can be shown as RFN = N * F + (N - 1), simplifying to NF - N + 1. Therefore we can see that the combined receptive field of N layers with size FxF matches a singel conv layer sized (NF - N + 1) x (NF - N + 1).\n",
    "\n",
    "Example with three layers of 5x5 filters:\n",
    "\n",
    "RF1 = F = 5\n",
    "RF2 = RF1 + (F-1) = 5 + 4 = 9\n",
    "RF3 = RF2 + (F-1) = 9 + 4 = 13\n",
    "with N=3 and F=5:\n",
    "\n",
    "NF - N + 1 translates to 3 * 5 - 3 + 1 = 15 - 2 = 13\n",
    "The receptive field for three 5x5 layers = 13x13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWZHh02CYKKY"
   },
   "source": [
    "## 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3wfpKtdYMIo"
   },
   "source": [
    "### 1.4 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhQdXFq-YT0-"
   },
   "source": [
    "**Answer:**\n",
    "The overall idea of the inception model is increasing representation of features by the model. It achieves this through parallel convolution filters that have different sized, combined with pooling, and then concatenating these. The moduel can identify small localized deatures as well as large global ones. One features are extracted, pooling layer reduces the dimensions of the feature maps, allowinf the model to identiy even more abstract features.There is then a concatenation step that combines the feature maps so the model can learn from mutli scaled features. This overall architecute is especially efficient for image processing because of the diverse pattern scales in images. Using different filter sizes concurrently makes the model able to gather features present in different scales. This model can also improve netowrk dempth without significantly increasinf computation costs, largely through smaller convolutions to decrease dimensionality. This structure increases efficiency and accurary without overfititng. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK87FU3SYUlL"
   },
   "source": [
    "### 1.4 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEyspnrjYUlL"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "- Naive version:\n",
    "\n",
    "  - 1x1 convolutions (128): 32x32x128\n",
    "  - 3x3 convolutions (192): 32x32x192\n",
    "  - 5x5 convolutions (96): 32x32x96\n",
    "  - 3x3 max pooling: 32x32x256 (unchanged depth)\n",
    "  - **Filter concatenation: 128+192+96+256 = 672 \n",
    "  Output size = 32x32x672**\n",
    "\n",
    "- Dimension reductions:\n",
    "\n",
    "  - 1x1 convolutions (128): 32x32x128\n",
    "  - 1x1 convolutions (128): 32x32x128\n",
    "  - 3x3 convolutions (192): 32x32x192\n",
    "  - 1x1 convolutions (32): 32x32x32\n",
    "  - 5x5 convolutions (96): 32x32x96\n",
    "  - 3x3 max pooling: 32x32x256\n",
    "  - 1x1 convolutions (64): 32x32x64\n",
    "  - **Filter concatenation: 128 + 192 + 96 + 64 = 480 \n",
    "  Output size = 32x32x480**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jG-3Cc4YUsu"
   },
   "source": [
    "### 1.4 (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyBOCjizYUsv"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "- Naive version:\n",
    "\n",
    "  - 1x1 convolutions (128): 1x1x32x32x256x128 = 33,554,432\n",
    "  - 3x3 convolutions (192): 3x3x32x32x256x192 = 452,984,832\n",
    "  - 5x5 convolutions (96): 5x5x32x32x256x96 = 629,145,600\n",
    "  - 3x3 max pooling: 0 convolution operations\n",
    "  - **Total: 1,115,684,864**\n",
    "\n",
    "- Dimension reductions:\n",
    "\n",
    "  - 1x1 convolutions (128): 1x1x32x32x256x128 = 33,554,432\n",
    "  - 1x1 convolutions (128): 1x1x32x32x256x128 = 33,554,432\n",
    "  - 3x3 convolutions (192): 3x3x32x32x128x192 = 226,492,416\n",
    "  - 1x1 convolutions (32): 1x1x32x32x256x32 = 8,388,608\n",
    "  - 5x5 convolutions (96): 5x5x32x32x32x96 = 78,643,200\n",
    "  - 3x3 max pooling: 0 convolution operations\n",
    "  - 1x1 convolutions (64): 1x1x32x32x256x64 = 16,777,216\n",
    "  - **Total: 397,410,304**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyC53m8gYUzm"
   },
   "source": [
    "### 1.4 (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt7M2Fs_YUzn"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "The computational difference between the naive module and the moduel with dimensionality reduction is large. The naive model requires a large amount of convolutional actions, which requires a large computational expenditure. This is especially true when there is large convolution sizes, and even more so in deep models. When dimensionality reduction is added to the module there is a reduction in convolutions before increasing. This filters the depth of the tensor which reduceses the computational cost. Even with later compact laters adding to computation there is still an overall benefit from the beginning trim.\n",
    "\n",
    "Basd on the previous question the naive method had 1,115,684,864 computations, while the dimensionality-reduced module had 397,410,304. This is a difference of 718,274,560 computations by using dimensionality reduction. This is over a third of the computational power which is a significant reduction in the computational power required. This means that the plain naive module is much less efficient than with dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9XYBTwQYcJy"
   },
   "source": [
    "## 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb-CINl5YcJz"
   },
   "source": [
    "### 1.5 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCSH37G-YcJz"
   },
   "source": [
    "**Answer:**\n",
    "The biggest differnce between Faster R-CNN and Fast R-CNN is the way they ahndle regional proposal generation. The Fast R-CNN uses selective search method, which is time consuming while Faster R-CNN uses a more integrated approach, it used the Regional Proposal Network (RPN) which creates regional proposes directly within the network. This means that there is no need for external methods like selective search, this greatly speeds up computational time. In addition, Faster R-CNN uses common convolution feature extractors which improves efficiency. It also uses a refined loss function which imporoves accuracy and efficiency as compared to FAST R-CNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ7NiQajYcJz"
   },
   "source": [
    "### 1.5 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idnf3ADsYcJz"
   },
   "source": [
    "**Answer:**\n",
    "A RPN is a full convolution network that predicts boundaries for objects and objectness scores. Its main purpose is to creare regional proposlas for use by networks like Fast R-CNN. \n",
    "Its architecture takes an input of an intermediate feature map made by previous layers of the NN.For each location the RPN predicts potential anchors and scores that show how likely it is that each box has an object which is the objectness score. These bounding boxes are then used as regional proposals for the network. The anchors are predetermined boxes with different scales and aspect ratios. The network is trained to adjust eh anschors to match teh actual boundaries in the training images and classify them. These predictions from the feature map create regional proposals that can then be evaluated by the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWRyaZiMYcJz"
   },
   "source": [
    "### 1.5 (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzx3AJvFYcJz"
   },
   "source": [
    "**Answer:**\n",
    "image: https://www.pexels.com/photo/close-up-photo-of-clownfish-128756/\n",
    "\n",
    "\n",
    "The picture of the clownfish is def into the first convolution layers of the Faster R-CNN, and it creates an intermediate feature map. This map contains spatial detaisl and patterns, such as the orange and white strips, shape and the water background, seen in this image. Then there will be a small sliding window that scans the feature map, every spatial position is examined for object regions. At every potiion the RPN will create multiple anchors or defined bounding boxes. Because the goal here is the clownfish teh anchors will be shaped to fit the fish considering different scales and aspect ratios. Next is bounding box regression, in this step each anchor makes adjustments to its shape to make sure it is perfectyl aligned. In this example it may shift to make sure it fits the tail and fins perfectly. Each anchor is assigend an objectness score which shows the change that an anchor is capturing an object. If an achor is just capturing the water background it would have a low score as compared ot one that fits the fish well. After scores are done, redundances are filtered out. After this is done only the proposals with high objectness scores are retained.  The proposals are then sent to the object detection part of the Faster R-CNN wehre the fish region would be further classified and refined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuu8dqGUYcJz"
   },
   "source": [
    "### 1.5 (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sc2iYdo_YcJz"
   },
   "source": [
    "**Answer:**\n",
    "In order to address overlaps the model firsy starts with highest score and then calcultaes the intersection over Union (IoU) for every other proposal. This is a metric used to calculate the overlap between two bounding boxes. If teh IoU with the reference proposal is higher than the threshold set to indicate overlap than this proposal will be removed. The refrence proposal is kept and this is repeated with the next highest score that has not been removed. This is done until all proposals have been kept or removed.\n",
    "\n",
    "Example:\n",
    "Using the clownfish example. The RPN will create regional proposals, some of these may overlap aroufn say the clownfish's face, but they have different sizes, aspect rations, or positions. First the proposals will be sorted by objectness score. The highest scores that perfectly capure the clownfish become the reference. If another proposa; overlaps this reference proposal based on the IoU threshold, they will be removed (suppresed). This repeats and by the end of the process there will only be the most relevent regions of the image without too much overlap. Faster R-CNN NMS limits the proposals to around 2000 to make sure there is efficency."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
