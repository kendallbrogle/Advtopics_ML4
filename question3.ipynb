{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To copy this template: File -> Save a Copy in Drive\n",
        "\n",
        "***DISCLAIMER**: In case of any discrepancy in the assignment instruction, please refer to the `PDF` document.*"
      ],
      "metadata": {
        "id": "OMAdi9qgC-B9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3 - Weakly and Semi-Supervised Learning for Image Classification"
      ],
      "metadata": {
        "id": "NcDhlfqyBd6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1"
      ],
      "metadata": {
        "id": "-id00ye6CNLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "Weakly supervised pre training: models are trained using data labeled inprecisely. The labels arent manually generated, instead they come from another source. In this article the hashtags would be an example of weak supervision. They provide some information about the image but are not specific labels summarizing the image.\n",
        "\n",
        "Semi supervised pretraining: takes advantage of both labeled and unlabeled data. It uses a small number of labeled data combined with a large number of unlabeled data. The model will be trained on the labeled data and then improved using the unlabeled data. This is based off of the assumption that the unlabeled and labeled data have similar distributions so that the model can improve generalization\n",
        "\n",
        "Using the same data set (1B image set): The weakly super vised pre training using the hashtags that went with each image as I mentioned above. This was their noisy/ weak labels because they do not direclty summarize/ or label the data but are associated to individual pictures. The model then uses these broad data points to generalize.\n",
        "\n",
        "The semi supervised pre training, uses more specific labels for the images on a smaller subset. In this case they used 1.2 million labeled images in 1000 classes, to improve accruacy and generalization.\n",
        "They first used a teacher model on this training data to label the examples in the unlabeled data. They kept only the highest scored classes on each image. Then they rank those images based on their classification score and then further train and fine tune the student model.\n",
        "\n"
      ],
      "metadata": {
        "id": "4kineJbrfcg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2"
      ],
      "metadata": {
        "id": "e1D_yfneCWqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2(a)"
      ],
      "metadata": {
        "id": "5-swV-4F8pso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The hashtag supervised data is noisier than the datasets like ImageNet because it is likely for many hashtags to be irrelevent to the content of the photo, and they may not be speciifically including hashtags to summarize the photo content. To understand this noise the study introduced additional noise by sampling from the overall hashtags and replacing some of the original ones with these samples. This model was then used on the ImageNet model to check for classification accuracy.\n",
        "They found that introducing 10% noise had a less than 1% decrease in classificiation accruacy. At 25% noise increase was about a 2% decrease in accuracy.\n",
        "\n",
        "This indicates that labal noise is a minor issue when trained on such a large data set and based in figure three and four we can conclude that the models were fairly resiliant to to lavbel noise.\n",
        "\n"
      ],
      "metadata": {
        "id": "BX8Ns_Imf-BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2(b)"
      ],
      "metadata": {
        "id": "t4O1Y1gG8sVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "Resampling hashtag distribution during pre training is important to transfer learning because:\n",
        " Hashtags follow a Zipfian distribution which means that a few are very popular and the rest are very rare, this can then lead to the model overly recognzing features associated with the most common hashtags and doing worse on the rarer ones. In resampling the hashtags this can ensure the model sees a more balances set of the hashtags and reduce bias of the most popular. It can reduce the frequency of the most common and increase the frequence of less common ones. Resampling to learn generalized features that are not too hyper specific. It can also help adress imbalanced data and ensure that overfitting is avoided. Resampling can also help to create a model that is less sensitive to noise by making sure its not relying on the most common hashtags.\n",
        "\n"
      ],
      "metadata": {
        "id": "gLttsWNl8sVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3"
      ],
      "metadata": {
        "id": "TpMHZDOECjD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3(a)"
      ],
      "metadata": {
        "id": "Xu0xnp_a8t5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The student model uses the aggregated data set created from the teacher model's predictions of the unlabeled data. The student model is is pre trained on the noisy supervision data and tehn is later fine tuned on the initial unlabaled data to fix any labeling errors from the teacher model's predictions.\n",
        "The teacher model creates this data for the student by first being trained on labeled data and then making predictions based on sampled of the large batch of unlabled data.\n",
        "\n",
        "This is considered to be a type of distillation technique because of its methodology. The student learns from 'soft probabilities' from the teacher model's output layer. The student model is indireclty learning from the teacher model on unlabeled data to improve performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GRo9fm0tf-ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3(b)"
      ],
      "metadata": {
        "id": "Ri59wswo8wSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "In this model K represents the number of impages per class that will be used in training the student models. Taking a subset from a much larger set of unlabeled data. The selection of these images is based on teh condidence of the parent model in its predictions and will be adjusted based on the size of the data set. Specifically, K = 16k for data of size 100M, K = 8k for 50M and K = 4K for 25M.\n",
        "\n",
        "P is the variable for the number of partitions wher each trainign example can show up. If P>1 this means that a training example can be included in more than one list and creates a more balanced class distribution during training. It essenitally combats classes being under represented. This reduces the chance od some classes dominating the learning process. Which, can overall prevent overfitting and improve the student model. Higher values of P creates more diversity in the training set.\n"
      ],
      "metadata": {
        "id": "FF8uLB3J8wSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3(c)"
      ],
      "metadata": {
        "id": "l0afIfh_8wXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "The teacher model is trained on the unlabeled data which is then used to infer labels on the larger set of unlabeled data.\n",
        "The algorithm picks the top K exmaples for each target, the teacher model then gets a softmax predictor vector. Only the highest classes for each image are kept, which means the model can get multiple relevant classes for each image. So, yes the images can belong to more than one class if P>1.\n"
      ],
      "metadata": {
        "id": "DX6hArot8wXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3(d)"
      ],
      "metadata": {
        "id": "QC1HEHCe8wbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "As the value of K increases the studen model's accuracy improves because more diverse examples are being included which allows for the model to generalize better. The model is learning more features within the data which strengthens it. The accuracy then begins to decrease because as K increases there is a higher chance of increased noise associated with incorreclty labeled data. It could also be a result of overfitting or redundancies in the data, where new information is not being introduced and the model is just further learning from similiar data.\n",
        "\n"
      ],
      "metadata": {
        "id": "3YNAJLoU8wbg"
      }
    }
  ]
}